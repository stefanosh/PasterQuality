{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a07938b4",
   "metadata": {},
   "source": [
    "### Get additional metrics for models (additional PasterRepo Code - without modyfing the original code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "italian-greene",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to catch any changes to libraries without restarting the notebook kernel every time\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "REPO_DIR = os.path.abspath('..')  # path to the root of the repository\n",
    "sys.path.append(REPO_DIR)\n",
    "os.environ[\"PROJECT_DIR\"] = REPO_DIR\n",
    "import lib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_percentage_error, mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "458597ed",
   "metadata": {},
   "source": [
    "# Initialize notebook parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b0224a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_dir = os.environ[\"PROJECT_DIR\"]\n",
    "dataset_name = 'tabular_100_trials_32_batch_size'\n",
    "os.makedirs(f'{project_dir}/paster_scripts/report_metrics/{dataset_name}', exist_ok=True)\n",
    "output_folder_single_models = f'{project_dir}/output_pasterquality/{dataset_name}'\n",
    "\n",
    "n_seeds = 15"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "788ea0b8",
   "metadata": {},
   "source": [
    "### Calculate MAPE, RMSE, MAE, R2 metrics for each seed of each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8c31dc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_json = {}\n",
    "\n",
    "y_train = np.load(f'{project_dir}/data/{dataset_name}/y_train.npy')\n",
    "y_val = np.load(f'{project_dir}/data/{dataset_name}/y_val.npy')\n",
    "y_test = np.load(f'{project_dir}/data/{dataset_name}/y_test.npy')\n",
    "\n",
    "for model in sorted(os.listdir(output_folder_single_models)):\n",
    "    metrics_json[model] = {}\n",
    "    for folder in sorted(os.listdir(f'{output_folder_single_models}/{model}')):\n",
    "        if folder == 'tuned':\n",
    "            for seed_folder in sorted(os.listdir(f'{output_folder_single_models}/{model}/{folder}')):\n",
    "                if not seed_folder.endswith('.toml'):\n",
    "                    if model != 'node':\n",
    "                        p_test = np.load(f'{output_folder_single_models}/{model}/{folder}/{seed_folder}/p_test.npy')\n",
    "                        p_val = np.load(f'{output_folder_single_models}/{model}/{folder}/{seed_folder}/p_val.npy')\n",
    "                        p_train = np.load(f'{output_folder_single_models}/{model}/{folder}/{seed_folder}/p_train.npy')\n",
    "                        y_info = lib.load_pickle(f'{output_folder_single_models}/{model}/{folder}/{seed_folder}/y_info.pickle')\n",
    "\n",
    "                        # Convert predictions to the original scale based on y_info mean and std (extracted from y_train)\n",
    "                        p_test = p_test * y_info['std'] + y_info['mean']\n",
    "                        p_val = p_val * y_info['std'] + y_info['mean']\n",
    "                        p_train = p_train * y_info['std'] + y_info['mean']\n",
    "\n",
    "                        # Convert MAPE to percentage\n",
    "                        mape_train = mean_absolute_percentage_error(y_train, p_train) * 100\n",
    "                        mape_val = mean_absolute_percentage_error(y_val, p_val) * 100\n",
    "                        mape_test = mean_absolute_percentage_error(y_test, p_test) * 100\n",
    "                        \n",
    "                        rmse_train = np.sqrt(mean_squared_error(y_train, p_train))\n",
    "                        rmse_val = np.sqrt(mean_squared_error(y_val, p_val))\n",
    "                        rmse_test = np.sqrt(mean_squared_error(y_test, p_test))\n",
    "                        \n",
    "                        mae_train = mean_absolute_error(y_train, p_train)\n",
    "                        mae_val = mean_absolute_error(y_val, p_val)\n",
    "                        mae_test = mean_absolute_error(y_test, p_test)\n",
    "                        \n",
    "                        r2_train = r2_score(y_train, p_train)\n",
    "                        r2_val = r2_score(y_val, p_val)\n",
    "                        r2_test = r2_score(y_test, p_test)\n",
    "                        \n",
    "                        if model not in metrics_json:\n",
    "                            metrics_json[model] = {}\n",
    "                        if seed_folder not in metrics_json[model]:\n",
    "                            metrics_json[model][seed_folder] = {}\n",
    "                        metrics_json[model][seed_folder]['MAPE'] = {}\n",
    "                        metrics_json[model][seed_folder]['RMSE'] = {}\n",
    "                        metrics_json[model][seed_folder]['MAE'] = {}\n",
    "                        metrics_json[model][seed_folder]['R2'] = {}\n",
    "\n",
    "                        metrics_json[model][seed_folder]['MAPE']['train'] = mape_train\n",
    "                        metrics_json[model][seed_folder]['MAPE']['val'] = mape_val\n",
    "                        metrics_json[model][seed_folder]['MAPE']['test'] = mape_test\n",
    "                        metrics_json[model][seed_folder]['RMSE']['train'] = rmse_train\n",
    "                        metrics_json[model][seed_folder]['RMSE']['val'] = rmse_val\n",
    "                        metrics_json[model][seed_folder]['RMSE']['test'] = rmse_test\n",
    "                        metrics_json[model][seed_folder]['MAE']['train'] = mae_train\n",
    "                        metrics_json[model][seed_folder]['MAE']['val'] = mae_val\n",
    "                        metrics_json[model][seed_folder]['MAE']['test'] = mae_test\n",
    "                        metrics_json[model][seed_folder]['R2']['train'] = r2_train\n",
    "                        metrics_json[model][seed_folder]['R2']['val'] = r2_val\n",
    "                        metrics_json[model][seed_folder]['R2']['test'] = r2_test\n",
    "\n",
    "# Save metrics to json file\n",
    "json_file_path = f'{project_dir}/paster_scripts/report_metrics/{dataset_name}/detailed_metrics_per_model_metric_seed_set.json'\n",
    "with open(json_file_path, 'w') as f:\n",
    "    json.dump(metrics_json, f, indent=4, default=str)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a1975cb4",
   "metadata": {},
   "source": [
    "### Calculate mean and std, across all seeds of each models, for each metric per (train, val, test) set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "82e03d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_mean_std = {}\n",
    "\n",
    "for model, model_data in metrics_json.items():\n",
    "    metrics_mean_std[model] = {}\n",
    "    for seed_folder, seed_data in model_data.items():\n",
    "        for metric, metric_data in seed_data.items():\n",
    "            if metric not in metrics_mean_std[model]:\n",
    "                metrics_mean_std[model][metric] = {}\n",
    "            for set_name, values in metric_data.items():\n",
    "                if set_name not in metrics_mean_std[model][metric]:\n",
    "                    metrics_mean_std[model][metric][set_name] = {'mean': [], 'std': []}\n",
    "                metrics_mean_std[model][metric][set_name]['mean'].append(values)\n",
    "                metrics_mean_std[model][metric][set_name]['std'].append(values)\n",
    "\n",
    "# Calculate mean and std for each metric set\n",
    "for model, model_data in metrics_mean_std.items():\n",
    "    for metric, metric_data in model_data.items():\n",
    "        for set_name, set_data in metric_data.items():\n",
    "            set_data['mean'] = round(np.mean(set_data['mean']).item(),4)\n",
    "            set_data['std'] = round(np.std(set_data['std']).item(),4)\n",
    "\n",
    "# Save metrics to json file\n",
    "json_file_path = f'{project_dir}/paster_scripts/report_metrics/{dataset_name}/aggregated_metrics.json'\n",
    "with open(json_file_path, 'w') as f:\n",
    "    json.dump(metrics_mean_std, f, indent=4, default=str)\n",
    "\n",
    "\n",
    "# Create a DataFrame from the list of dictionaries for the aggregated metrics (mean and sted)\n",
    "metrics_list = []\n",
    "for model, model_data in metrics_mean_std.items():\n",
    "    for metric, metric_data in model_data.items():\n",
    "        for set_name, values in metric_data.items():\n",
    "            metrics_list.append({\n",
    "                'Model': model,\n",
    "                'Metric': metric,\n",
    "                'Set': set_name,\n",
    "                'Mean': values['mean'],\n",
    "                'Std': values['std']\n",
    "            })\n",
    "df = pd.DataFrame(metrics_list)\n",
    "df.to_csv(f'{project_dir}/paster_scripts/report_metrics/{dataset_name}/aggregated_metrics.csv', index=False)\n",
    "df = df.sort_values(by=['Metric', 'Set', 'Mean'], ascending=[True, True, True]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b23e3771",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Metric</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MAPE</th>\n",
       "      <th>R2</th>\n",
       "      <th>RMSE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>resnet</th>\n",
       "      <td>2.1470</td>\n",
       "      <td>12.1941</td>\n",
       "      <td>0.8409</td>\n",
       "      <td>2.6966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ft_transformer</th>\n",
       "      <td>2.1387</td>\n",
       "      <td>12.1707</td>\n",
       "      <td>0.8400</td>\n",
       "      <td>2.7050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>2.1967</td>\n",
       "      <td>12.5720</td>\n",
       "      <td>0.8336</td>\n",
       "      <td>2.7570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>autoint</th>\n",
       "      <td>2.2335</td>\n",
       "      <td>12.1982</td>\n",
       "      <td>0.8155</td>\n",
       "      <td>2.8925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dcn2</th>\n",
       "      <td>2.2403</td>\n",
       "      <td>12.9191</td>\n",
       "      <td>0.8065</td>\n",
       "      <td>2.9626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>snn</th>\n",
       "      <td>2.2846</td>\n",
       "      <td>12.6111</td>\n",
       "      <td>0.7954</td>\n",
       "      <td>3.0403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tabnet</th>\n",
       "      <td>2.5719</td>\n",
       "      <td>14.4862</td>\n",
       "      <td>0.7752</td>\n",
       "      <td>3.1880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>grownet</th>\n",
       "      <td>2.5506</td>\n",
       "      <td>14.1089</td>\n",
       "      <td>0.7643</td>\n",
       "      <td>3.2740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lightgbm_</th>\n",
       "      <td>2.2907</td>\n",
       "      <td>11.5963</td>\n",
       "      <td>0.7335</td>\n",
       "      <td>3.4906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgboost_</th>\n",
       "      <td>2.4815</td>\n",
       "      <td>12.8348</td>\n",
       "      <td>0.6951</td>\n",
       "      <td>3.7192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>catboost_</th>\n",
       "      <td>2.5978</td>\n",
       "      <td>13.2576</td>\n",
       "      <td>0.6704</td>\n",
       "      <td>3.8787</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Metric             MAE     MAPE      R2    RMSE\n",
       "Model                                          \n",
       "resnet          2.1470  12.1941  0.8409  2.6966\n",
       "ft_transformer  2.1387  12.1707  0.8400  2.7050\n",
       "mlp             2.1967  12.5720  0.8336  2.7570\n",
       "autoint         2.2335  12.1982  0.8155  2.8925\n",
       "dcn2            2.2403  12.9191  0.8065  2.9626\n",
       "snn             2.2846  12.6111  0.7954  3.0403\n",
       "tabnet          2.5719  14.4862  0.7752  3.1880\n",
       "grownet         2.5506  14.1089  0.7643  3.2740\n",
       "lightgbm_       2.2907  11.5963  0.7335  3.4906\n",
       "xgboost_        2.4815  12.8348  0.6951  3.7192\n",
       "catboost_       2.5978  13.2576  0.6704  3.8787"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transpose the DataFrame to have the metrics as columns and the models as rows, only for the test set\n",
    "df_test = df[df['Set'] == 'val']\n",
    "df_test = df_test.pivot(index='Model', columns='Metric', values='Mean')\n",
    "df_test.sort_values(by=['RMSE'], ascending=[True], inplace=True)\n",
    "df_test\n",
    "# df_test.to_latex(f'{project_dir}/paster_scripts/report_metrics/{dataset_name}/aggregated_metrics_test.tex', index=True, float_format=\"%.2f\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8f8ce75c",
   "metadata": {},
   "source": [
    "### Create ensemble groups and calculate their metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad9084ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each model, obtain three ensembles by splitting the 15 single models (seeds) into three disjoint groups of equal size and \n",
    "# averaging predictions of single models within each group.\n",
    "\n",
    "y_train = np.load(f'{project_dir}/data/{dataset_name}/y_train.npy')\n",
    "y_val = np.load(f'{project_dir}/data/{dataset_name}/y_val.npy')\n",
    "y_test = np.load(f'{project_dir}/data/{dataset_name}/y_test.npy')\n",
    "\n",
    "ensemble_metrics = {}\n",
    "\n",
    "for model in sorted(os.listdir(output_folder_single_models)):\n",
    "\n",
    "    # Skip node model\n",
    "    if model == 'node':\n",
    "        continue\n",
    "   \n",
    "    if model not in ensemble_metrics:\n",
    "        ensemble_metrics[model] = {}\n",
    "    \n",
    "    for folder in sorted(os.listdir(f'{output_folder_single_models}/{model}')):\n",
    "        if folder == 'tuned':\n",
    "            for seeds in [range(0, 5), range(5, 10), range(10, 15)]:\n",
    "                train_pred_list = []\n",
    "                val_pred_list = []\n",
    "                test_pred_list = []\n",
    "                for seed in seeds:\n",
    "                    if not folder.endswith('.toml'):\n",
    "                            p_test = np.load(f'{output_folder_single_models}/{model}/{folder}/{seed}/p_test.npy')\n",
    "                            p_val = np.load(f'{output_folder_single_models}/{model}/{folder}/{seed}/p_val.npy')\n",
    "                            p_train = np.load(f'{output_folder_single_models}/{model}/{folder}/{seed}/p_train.npy')\n",
    "                            y_info = lib.load_pickle(f'{output_folder_single_models}/{model}/{folder}/{seed}/y_info.pickle')\n",
    "\n",
    "                            # Convert predictions to the original scale based on y_info mean and std (extracted from y_train)\n",
    "                            p_test = p_test * y_info['std'] + y_info['mean']\n",
    "                            p_val = p_val * y_info['std'] + y_info['mean']\n",
    "                            p_train = p_train * y_info['std'] + y_info['mean']\n",
    "\n",
    "                            train_pred_list.append(p_train)\n",
    "                            val_pred_list.append(p_val)\n",
    "                            test_pred_list.append(p_test)\n",
    "                \n",
    "                #Taking the average of this seed_ensemble group\n",
    "                train_pred_list = np.array(train_pred_list)\n",
    "                val_pred_list = np.array(val_pred_list)\n",
    "                test_pred_list = np.array(test_pred_list)\n",
    "                train_pred_list = np.mean(train_pred_list, axis=0)\n",
    "                val_pred_list = np.mean(val_pred_list, axis=0)\n",
    "                test_pred_list = np.mean(test_pred_list, axis=0)\n",
    "\n",
    "                # Convert MAPE to percentage\n",
    "                mape_train = mean_absolute_percentage_error(y_train, train_pred_list) * 100\n",
    "                mape_val = mean_absolute_percentage_error(y_val, val_pred_list) * 100\n",
    "                mape_test = mean_absolute_percentage_error(y_test, test_pred_list) * 100\n",
    "\n",
    "                rmse_train = np.sqrt(mean_squared_error(y_train, train_pred_list))\n",
    "                rmse_val = np.sqrt(mean_squared_error(y_val, val_pred_list))\n",
    "                rmse_test = np.sqrt(mean_squared_error(y_test, test_pred_list))\n",
    "\n",
    "                mae_train = mean_absolute_error(y_train, train_pred_list)\n",
    "                mae_val = mean_absolute_error(y_val, val_pred_list)\n",
    "                mae_test = mean_absolute_error(y_test, test_pred_list)\n",
    "\n",
    "                r2_train = r2_score(y_train, train_pred_list)\n",
    "                r2_val = r2_score(y_val, val_pred_list)\n",
    "                r2_test = r2_score(y_test, test_pred_list)\n",
    "\n",
    "                # Save metrics to json file for this model and this seed_ensemble group\n",
    "                group = f'{min(seeds)}_{max(seeds)}'\n",
    "                ensemble_metrics[model][group] = {}\n",
    "                ensemble_metrics[model][group]['MAPE'] = {}\n",
    "                ensemble_metrics[model][group]['RMSE'] = {}\n",
    "                ensemble_metrics[model][group]['MAE'] = {}\n",
    "                ensemble_metrics[model][group]['R2'] = {}\n",
    "\n",
    "                ensemble_metrics[model][group]['MAPE']['train'] = mape_train\n",
    "                ensemble_metrics[model][group]['MAPE']['val'] = mape_val\n",
    "                ensemble_metrics[model][group]['MAPE']['test'] = mape_test\n",
    "\n",
    "                ensemble_metrics[model][group]['RMSE']['train'] = rmse_train\n",
    "                ensemble_metrics[model][group]['RMSE']['val'] = rmse_val\n",
    "                ensemble_metrics[model][group]['RMSE']['test'] = rmse_test\n",
    "\n",
    "                ensemble_metrics[model][group]['MAE']['train'] = mae_train\n",
    "                ensemble_metrics[model][group]['MAE']['val'] = mae_val\n",
    "                ensemble_metrics[model][group]['MAE']['test'] = mae_test\n",
    "\n",
    "                ensemble_metrics[model][group]['R2']['train'] = r2_train\n",
    "                ensemble_metrics[model][group]['R2']['val'] = r2_val\n",
    "                ensemble_metrics[model][group]['R2']['test'] = r2_test\n",
    "\n",
    "# Save metrics to json file\n",
    "json_file_path = f'{project_dir}/paster_scripts/report_metrics/{dataset_name}/ensemble_detailed_metrics.json'\n",
    "with open(json_file_path, 'w') as f:\n",
    "    json.dump(ensemble_metrics, f, indent=4, default=str)\n",
    "\n",
    "# Create a DataFrame from the list of dictionaries for the ensemble_metrics (mean and sted)\n",
    "ensemble_metrics_list = []\n",
    "for model, model_data in ensemble_metrics.items():\n",
    "    for group, group_data in model_data.items():\n",
    "        for metric, metric_data in group_data.items():\n",
    "            for set_name, values in metric_data.items():\n",
    "                ensemble_metrics_list.append({\n",
    "                    'Model': model,\n",
    "                    'Group': group,\n",
    "                    'Metric': metric,\n",
    "                    'Set': set_name,\n",
    "                    'Value': values\n",
    "                })\n",
    "\n",
    "df_detailed_metrics_ensemble = pd.DataFrame(ensemble_metrics_list)\n",
    "df_detailed_metrics_ensemble.to_csv(f'{project_dir}/paster_scripts/report_metrics/{dataset_name}/ensemble_detailed_metrics.csv', index=False)\n",
    "df_detailed_metrics_ensemble = df_detailed_metrics_ensemble.sort_values(by=['Metric', 'Set', 'Value'], ascending=[True, True, True])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "52d3bd73",
   "metadata": {},
   "source": [
    "### Calculate the mean and std for the metrics between the ensemble groups of each model and as always for each (train, val, test) set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0370b619",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ensemble_metrics\n",
    "model_results = {}\n",
    "for model, groups in data.items():\n",
    "    model_results[model] = {}\n",
    "    for group, metrics in groups.items():\n",
    "        for metric, values in metrics.items():\n",
    "            if metric not in model_results[model]:\n",
    "                model_results[model][metric] = {}\n",
    "            for set_name, value in values.items():\n",
    "                if set_name not in model_results[model][metric]:\n",
    "                    model_results[model][metric][set_name] = []\n",
    "                model_results[model][metric][set_name].append(value)\n",
    "\n",
    "mean_std_results = {}\n",
    "for model, metrics in model_results.items():\n",
    "    mean_std_results[model] = {}\n",
    "    for metric, sets in metrics.items():\n",
    "        mean_std_results[model][metric] = {}\n",
    "        for set_name, values in sets.items():\n",
    "            mean = round(np.mean(values),4)\n",
    "            std_dev = round(np.std(values),4)\n",
    "            mean_std_results[model][metric][set_name] = {\n",
    "                \"mean\": mean,\n",
    "                \"std_dev\": std_dev\n",
    "            }\n",
    "\n",
    "# Save metrics to json file\n",
    "json_file_path = f'{project_dir}/paster_scripts/report_metrics/{dataset_name}/ensemble_aggregated_metrics.json'\n",
    "with open(json_file_path, 'w') as f:\n",
    "    json.dump(mean_std_results, f, indent=4, default=str)\n",
    "\n",
    "# Create a DataFrame from the list of dictionaries for the aggregated ensemble_metrics (mean and sted)\n",
    "ensemble_metrics_list = []\n",
    "for model, model_data in mean_std_results.items():\n",
    "    for metric, metric_data in model_data.items():\n",
    "        for set_name, values in metric_data.items():\n",
    "            ensemble_metrics_list.append({\n",
    "                'Model': model,\n",
    "                'Metric': metric,\n",
    "                'Set': set_name,\n",
    "                'Mean': values['mean'],\n",
    "                'Std': values['std_dev']\n",
    "            })\n",
    "\n",
    "df_aggregated_metrics_ensemble = pd.DataFrame(ensemble_metrics_list)\n",
    "df_aggregated_metrics_ensemble.to_csv(f'{project_dir}/paster_scripts/report_metrics/{dataset_name}/ensemble_aggregated_metrics.csv', index=False)\n",
    "df_aggregated_metrics_ensemble = df_aggregated_metrics_ensemble.sort_values(by=['Metric', 'Set', 'Mean'], ascending=[True, True, True])\n",
    "df_aggregated_metrics_ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2cbe679b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transpose the DataFrame to have the metrics as columns and the models as rows, only for the test set\n",
    "# also keep the std in parenthesis next to the mean\n",
    "\n",
    "tmpp = df_aggregated_metrics_ensemble.copy()\n",
    "tmpp = tmpp[tmpp['Set'] == 'test']\n",
    "tmpp = tmpp.pivot(index='Model', columns='Metric', values='Mean')\n",
    "tmpp.to_latex(f'{project_dir}/paster_scripts/report_metrics/{dataset_name}/ensemble_aggregated_metrics_test.tex', index=True, float_format=\"%.2f\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "103cc05d",
   "metadata": {},
   "source": [
    "### Create an ensemble by mixing the top five performing ensemble groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24915360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# just to have one idea in the test\n",
    "# ens_df[(ens_df['Metric'] == 'RMSE') & (ens_df['Set'] == 'test')].sort_values(by=['Value'], ascending=[True]).head(pool_of_models)\n",
    "\n",
    "# Create an ensemble by mixing the top X performing ensemble groups in the validation (val) set\n",
    "pool_of_models = 20\n",
    "ens_df = df_detailed_metrics_ensemble.copy()\n",
    "ens_mape = ens_df[(ens_df['Metric'] == 'MAPE') & (ens_df['Set'] == 'val')].sort_values(by=['Value'], ascending=[True]).head(pool_of_models)\n",
    "ens_r2 = ens_df[(ens_df['Metric'] == 'R2') & (ens_df['Set'] == 'val')].sort_values(by=['Value'], ascending=[False]).head(pool_of_models)\n",
    "ens_mae = ens_df[(ens_df['Metric'] == 'MAE') & (ens_df['Set'] == 'val')].sort_values(by=['Value'], ascending=[True]).head(pool_of_models)\n",
    "ens_rmse = ens_df[(ens_df['Metric'] == 'RMSE') & (ens_df['Set'] == 'val')].sort_values(by=['Value'], ascending=[True]).head(pool_of_models)\n",
    "\n",
    "# Merge the four DataFrames to find the common model names and groups\n",
    "common_df = ens_mape.merge(ens_r2, on=['Model', 'Group'], how='inner')\n",
    "common_df = common_df.merge(ens_mae, on=['Model', 'Group'], how='inner')\n",
    "common_df = common_df.merge(ens_rmse, on=['Model', 'Group'], how='inner')\n",
    "\n",
    "# TODO: continue this\n",
    "# Inspect the common_df and choose models and their ensemble groups to put in the models_to_ensemble\n",
    "models_to_ensemble = {}\n",
    "models_to_ensemble['dcn2'] = \"10_14\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b744b47f",
   "metadata": {},
   "source": [
    "### Create ensemble from the top x performing single models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9832b189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Try with three or five models\n",
    "# TODO: simpliy above cells with for loops, based on this one\n",
    "# TODO: replace the multiple lines for metrics assignment to json, with for loops\n",
    "# TODO: find for each model in models_to_ensemble, the best seed (with the best metrics in the val set)\n",
    "#       compare the results with the random (e.g. first seed)\n",
    "# TODO: in the first cell of the notebook find also the best performing seed for each model\n",
    "\n",
    "# Inspect below, to find the models to ensemble\n",
    "# df[(df['Metric'] == 'MAE') & (df['Set'] == 'val')].sort_values(by=['Mean'], ascending=[True])\n",
    "\n",
    "# models_to_ensemble = [\"dcn2\", \"catboost_\", \"resnet\", \"autoint\", \"mlp\"]\n",
    "# models_to_ensemble = [\"ft_transformer\", \"catboost_\", \"snn\", \"mlp\", \"resnet\"]\n",
    "models_to_ensemble = [\"ft_transformer\", \"dcn2\", \"autoint\", \"mlp\", \"resnet\"]\n",
    "\n",
    "seed_folder = 0 #(take predictions from 0.toml)\n",
    "folder = 'tuned'\n",
    "\n",
    "train_predictions = []\n",
    "val_predictions = []\n",
    "test_predictions = []\n",
    "\n",
    "y_train = np.load(f'{project_dir}/data/{dataset_name}/y_train.npy')\n",
    "y_val = np.load(f'{project_dir}/data/{dataset_name}/y_val.npy')\n",
    "y_test = np.load(f'{project_dir}/data/{dataset_name}/y_test.npy')\n",
    "\n",
    "for model in sorted(os.listdir(output_folder_single_models)):\n",
    "    if model in models_to_ensemble:\n",
    "        p_test = np.load(f'{output_folder_single_models}/{model}/{folder}/{seed_folder}/p_test.npy')\n",
    "        p_val = np.load(f'{output_folder_single_models}/{model}/{folder}/{seed_folder}/p_val.npy')\n",
    "        p_train = np.load(f'{output_folder_single_models}/{model}/{folder}/{seed_folder}/p_train.npy')\n",
    "        y_info = lib.load_pickle(f'{output_folder_single_models}/{model}/{folder}/{seed_folder}/y_info.pickle')\n",
    "\n",
    "        # Convert predictions to the original scale based on y_info mean and std (extracted from y_train)\n",
    "        p_test = p_test * y_info['std'] + y_info['mean']\n",
    "        p_val = p_val * y_info['std'] + y_info['mean']\n",
    "        p_train = p_train * y_info['std'] + y_info['mean']\n",
    "\n",
    "        train_predictions.append(p_train)\n",
    "        val_predictions.append(p_val)\n",
    "        test_predictions.append(p_test)\n",
    "\n",
    "#Taking the average of this seed_ensemble group\n",
    "train_predictions_orig = np.array(train_predictions)\n",
    "val_predictions_orig = np.array(val_predictions)\n",
    "test_predictions_orig = np.array(test_predictions)\n",
    "train_predictions = np.mean(train_predictions_orig, axis=0)\n",
    "val_predictions = np.mean(val_predictions_orig, axis=0)\n",
    "test_predictions = np.mean(test_predictions_orig, axis=0)\n",
    "\n",
    "# Convert MAPE to percentage\n",
    "mape_train = mean_absolute_percentage_error(y_train, train_predictions) * 100\n",
    "mape_val = mean_absolute_percentage_error(y_val, val_predictions) * 100\n",
    "mape_test = mean_absolute_percentage_error(y_test, test_predictions) * 100\n",
    "\n",
    "rmse_train = np.sqrt(mean_squared_error(y_train, train_predictions))\n",
    "rmse_val = np.sqrt(mean_squared_error(y_val, val_predictions))\n",
    "rmse_test = np.sqrt(mean_squared_error(y_test, test_predictions))\n",
    "\n",
    "mae_train = mean_absolute_error(y_train, train_predictions)\n",
    "mae_val = mean_absolute_error(y_val, val_predictions)\n",
    "mae_test = mean_absolute_error(y_test, test_predictions)\n",
    "\n",
    "r2_train = r2_score(y_train, train_predictions)\n",
    "r2_val = r2_score(y_val, val_predictions)\n",
    "r2_test = r2_score(y_test, test_predictions)\n",
    "\n",
    "# Save metrics to json file\n",
    "json_file_path = f'{project_dir}/paster_scripts/report_metrics/{dataset_name}/ensemble_top_single_models_metrics.json'\n",
    "with open(json_file_path, 'w') as f:\n",
    "    json.dump({'MAPE': {'train': mape_train, 'val': mape_val, 'test': mape_test}, \n",
    "               'RMSE': {'train': rmse_train, 'val': rmse_val, 'test': rmse_test},\n",
    "               'MAE':  {'train': mae_train, 'val': mae_val, 'test': mae_test},\n",
    "               'R2':   {'train': r2_train, 'val': r2_val, 'test': r2_test},\n",
    "               \"models:\" : models_to_ensemble}, f, indent=4, default=str)    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1899b250",
   "metadata": {},
   "source": [
    "## Create ensemble with the above and a default RF or XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74debaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error, mean_absolute_error\n",
    "import xgboost as xgb\n",
    "\n",
    "# model = RandomForestRegressor()\n",
    "model = xgb.XGBRegressor()\n",
    "\n",
    "y_train = np.load(f'{project_dir}/data/{dataset_name}/y_train.npy')\n",
    "y_val = np.load(f'{project_dir}/data/{dataset_name}/y_val.npy')\n",
    "y_test = np.load(f'{project_dir}/data/{dataset_name}/y_test.npy')\n",
    "\n",
    "x_train = np.load(f'{project_dir}/data/{dataset_name}/N_train.npy')\n",
    "x_val = np.load(f'{project_dir}/data/{dataset_name}/N_val.npy')\n",
    "x_test = np.load(f'{project_dir}/data/{dataset_name}/N_test.npy')\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "### Check from here and below, kati paizei lathos\n",
    "# Get predictions for all sets\n",
    "y_pred_train = model.predict(x_train)\n",
    "y_pred_val = model.predict(x_val)\n",
    "y_pred_test = model.predict(x_test)\n",
    "\n",
    "train_predictions_orig = np.insert(train_predictions_orig, 0, y_pred_train, axis=0)\n",
    "val_predictions_orig = np.insert(val_predictions_orig, 0, y_pred_val, axis=0)\n",
    "test_predictions_orig = np.insert(test_predictions_orig, 0, y_pred_test, axis=0)\n",
    "\n",
    "#Taking the average of this ensemble group\n",
    "train_predictions = np.mean(train_predictions_orig, axis=0)\n",
    "val_predictions = np.mean(val_predictions_orig, axis=0)\n",
    "test_predictions = np.mean(test_predictions_orig, axis=0)\n",
    "\n",
    "#### Metrics with all models together with RF\n",
    "mape_train = mean_absolute_percentage_error(y_train, y_pred_train) * 100\n",
    "mape_val = mean_absolute_percentage_error(y_val, y_pred_val) * 100\n",
    "mape_test = mean_absolute_percentage_error(y_test, y_pred_test) * 100\n",
    "\n",
    "rmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "rmse_val = np.sqrt(mean_squared_error(y_val, y_pred_val))\n",
    "rmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "\n",
    "mae_train = mean_absolute_error(y_train, y_pred_train)\n",
    "mae_val = mean_absolute_error(y_val, y_pred_val)\n",
    "mae_test = mean_absolute_error(y_test, y_pred_test)\n",
    "\n",
    "r2_train = r2_score(y_train, y_pred_train)\n",
    "r2_val = r2_score(y_val, y_pred_val)\n",
    "r2_test = r2_score(y_test, y_pred_test)\n",
    "\n",
    "\n",
    "##### metrics for RF alone\n",
    "rf_mape_train = mean_absolute_percentage_error(y_train, train_predictions) * 100\n",
    "rf_mape_val = mean_absolute_percentage_error(y_val, val_predictions) * 100\n",
    "rf_mape_test = mean_absolute_percentage_error(y_test, test_predictions) * 100\n",
    "\n",
    "rf_rmse_train = np.sqrt(mean_squared_error(y_train, train_predictions))\n",
    "rf_rmse_val = np.sqrt(mean_squared_error(y_val, val_predictions))\n",
    "rf_rmse_test = np.sqrt(mean_squared_error(y_test, test_predictions))\n",
    "\n",
    "rf_mae_train = mean_absolute_error(y_train, train_predictions)\n",
    "rf_mae_val = mean_absolute_error(y_val, val_predictions)\n",
    "rf_mae_test = mean_absolute_error(y_test, test_predictions)\n",
    "\n",
    "rf_r2_train = r2_score(y_train, train_predictions)\n",
    "rf_r2_val = r2_score(y_val, val_predictions)\n",
    "rf_r2_test = r2_score(y_test, test_predictions)\n",
    "\n",
    "# Save metrics to json file\n",
    "json_file_path = f'{project_dir}/paster_scripts/report_metrics/{dataset_name}/ensemble_top_single_models_RF_metrics.json'\n",
    "with open(json_file_path, 'w') as f:\n",
    "    json.dump({\"ensemble_with_RF\": {\n",
    "                    'MAPE': {'train': mape_train, 'val': mape_val, 'test': mape_test}, \n",
    "                    'RMSE': {'train': rmse_train, 'val': rmse_val, 'test': rmse_test},\n",
    "                    'MAE':  {'train': mae_train, 'val': mae_val, 'test': mae_test},\n",
    "                    'R2':   {'train': r2_train, 'val': r2_val, 'test': r2_test}},\n",
    "                    \"models:\" : models_to_ensemble.append(\"RF\"),\n",
    "                \"RF_alone\": {\n",
    "                    'MAPE': {'train': rf_mape_train, 'val': rf_mape_val, 'test': rf_mape_test},\n",
    "                    'RMSE': {'train': rf_rmse_train, 'val': rf_rmse_val, 'test': rf_rmse_test},\n",
    "                    'MAE':  {'train': rf_mae_train, 'val': rf_mae_val, 'test': rf_mae_test},\n",
    "                    'R2':   {'train': rf_r2_train, 'val': rf_r2_val, 'test': rf_r2_test}}}, f, indent=4, default=str)\n",
    "    \n",
    "\n",
    "with open(json_file_path) as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "ensemble_data = data['ensemble_with_RF']\n",
    "rf_alone_data = data['RF_alone']\n",
    "\n",
    "ensemble_df = pd.DataFrame(ensemble_data).T\n",
    "rf_alone_df = pd.DataFrame(rf_alone_data).T\n",
    "\n",
    "ensemble_df.columns = [f'ensemble_{col}' for col in ensemble_df.columns]\n",
    "rf_alone_df.columns = [f'rf_alone_{col}' for col in rf_alone_df.columns]\n",
    "\n",
    "comparison_df = pd.concat([ensemble_df, rf_alone_df], axis=1)\n",
    "comparison_df.to_csv(f'{project_dir}/paster_scripts/report_metrics/{dataset_name}/ensemble_top_single_models_RF_metrics.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b21e5e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ensemble_train</th>\n",
       "      <th>ensemble_val</th>\n",
       "      <th>ensemble_test</th>\n",
       "      <th>rf_alone_train</th>\n",
       "      <th>rf_alone_val</th>\n",
       "      <th>rf_alone_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MAPE</th>\n",
       "      <td>0.005711</td>\n",
       "      <td>13.150758</td>\n",
       "      <td>12.12141</td>\n",
       "      <td>6.337018</td>\n",
       "      <td>10.480887</td>\n",
       "      <td>9.234461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMSE</th>\n",
       "      <td>0.0014376971</td>\n",
       "      <td>4.5154862</td>\n",
       "      <td>3.8829393</td>\n",
       "      <td>1.4745461</td>\n",
       "      <td>2.4618459</td>\n",
       "      <td>2.6997578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAE</th>\n",
       "      <td>0.0009550007</td>\n",
       "      <td>2.647571</td>\n",
       "      <td>2.3295279</td>\n",
       "      <td>1.1214733</td>\n",
       "      <td>1.8887786</td>\n",
       "      <td>1.722553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.554805</td>\n",
       "      <td>0.793707</td>\n",
       "      <td>0.957771</td>\n",
       "      <td>0.867669</td>\n",
       "      <td>0.900273</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ensemble_train ensemble_val ensemble_test rf_alone_train rf_alone_val  \\\n",
       "MAPE       0.005711    13.150758      12.12141       6.337018    10.480887   \n",
       "RMSE   0.0014376971    4.5154862     3.8829393      1.4745461    2.4618459   \n",
       "MAE    0.0009550007     2.647571     2.3295279      1.1214733    1.8887786   \n",
       "R2              1.0     0.554805      0.793707       0.957771     0.867669   \n",
       "\n",
       "     rf_alone_test  \n",
       "MAPE      9.234461  \n",
       "RMSE     2.6997578  \n",
       "MAE       1.722553  \n",
       "R2        0.900273  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparison_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
